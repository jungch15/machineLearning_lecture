{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":310927,"sourceType":"datasetVersion","datasetId":130081},{"sourceId":7835300,"sourceType":"datasetVersion","datasetId":4592648}],"dockerImageVersionId":30627,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-03-20T07:34:57.229681Z","iopub.execute_input":"2024-03-20T07:34:57.230107Z","iopub.status.idle":"2024-03-20T07:34:59.263215Z","shell.execute_reply.started":"2024-03-20T07:34:57.230076Z","shell.execute_reply":"2024-03-20T07:34:59.262254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport json\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm, tqdm_notebook\nimport random\nimport glob\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom torch.nn.modules.loss import _WeightedLoss\nimport torch.nn.functional as F\nfrom numpy.random import seed\nimport torchvision\nfrom torchvision import transforms # 이미지 데이터 transform","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.265676Z","iopub.execute_input":"2024-03-20T07:34:59.265974Z","iopub.status.idle":"2024-03-20T07:34:59.271960Z","shell.execute_reply.started":"2024-03-20T07:34:59.265949Z","shell.execute_reply":"2024-03-20T07:34:59.271123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nrandom.seed(42)\nnp.random.seed(42)\ntorch.manual_seed(42)\nif device =='cuda':\n    torch.cuda.manual_seed(42)\n    torch.cuda.manual_seed_all(42)\n\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.273034Z","iopub.execute_input":"2024-03-20T07:34:59.273820Z","iopub.status.idle":"2024-03-20T07:34:59.321678Z","shell.execute_reply.started":"2024-03-20T07:34:59.273794Z","shell.execute_reply":"2024-03-20T07:34:59.320896Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data preprocessing","metadata":{}},{"cell_type":"code","source":"data_dir = '/kaggle/input/best-artworks-of-all-time/'\ndata = pd.read_csv('/kaggle/input/best-artworks-of-all-time/artists.csv')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.322806Z","iopub.execute_input":"2024-03-20T07:34:59.323044Z","iopub.status.idle":"2024-03-20T07:34:59.372502Z","shell.execute_reply.started":"2024-03-20T07:34:59.323023Z","shell.execute_reply":"2024-03-20T07:34:59.371867Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_df = data.groupby('genre')['paintings'].sum().reset_index(name='Count')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.373339Z","iopub.execute_input":"2024-03-20T07:34:59.373564Z","iopub.status.idle":"2024-03-20T07:34:59.391878Z","shell.execute_reply.started":"2024-03-20T07:34:59.373544Z","shell.execute_reply":"2024-03-20T07:34:59.391036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_df[genre_df['Count']>=150].reset_index().sort_values(by='Count', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.393432Z","iopub.execute_input":"2024-03-20T07:34:59.393701Z","iopub.status.idle":"2024-03-20T07:34:59.414183Z","shell.execute_reply.started":"2024-03-20T07:34:59.393678Z","shell.execute_reply":"2024-03-20T07:34:59.413275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre = np.array(genre_df[genre_df['Count']>=200].reset_index()['genre'])\nlen(genre)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.415281Z","iopub.execute_input":"2024-03-20T07:34:59.415553Z","iopub.status.idle":"2024-03-20T07:34:59.423040Z","shell.execute_reply.started":"2024-03-20T07:34:59.415528Z","shell.execute_reply":"2024-03-20T07:34:59.421511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre = ['Baroque', 'Cubism', 'High Renaissance',\n        'Impressionism', 'Expressionism', 'Northern Renaissance', 'Pop Art', \n         'Post-Impressionism', 'Primitivism', 'Romanticism', 'Surrealism', 'Symbolism']\ngenre_list = [x for x in genre]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.423965Z","iopub.execute_input":"2024-03-20T07:34:59.424325Z","iopub.status.idle":"2024-03-20T07:34:59.434582Z","shell.execute_reply.started":"2024-03-20T07:34:59.424291Z","shell.execute_reply":"2024-03-20T07:34:59.433849Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ngenre_artist = defaultdict(list)\nfor g in genre:\n    for i in range(len(data)):\n        if data['genre'].iloc[i] == g:\n            genre_artist[g].append(data['name'].iloc[i])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.435565Z","iopub.execute_input":"2024-03-20T07:34:59.435810Z","iopub.status.idle":"2024-03-20T07:34:59.454523Z","shell.execute_reply.started":"2024-03-20T07:34:59.435788Z","shell.execute_reply":"2024-03-20T07:34:59.453829Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_artist\n# genre마다 최대 4명 적으면 1명\n#4명이 4개, 3명이 2, 2명이 2, 1명이 4","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.455389Z","iopub.execute_input":"2024-03-20T07:34:59.455932Z","iopub.status.idle":"2024-03-20T07:34:59.466882Z","shell.execute_reply.started":"2024-03-20T07:34:59.455908Z","shell.execute_reply":"2024-03-20T07:34:59.466104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir image_dir\nfor g in genre:\n    gg = g.replace(\" \", \"_\")\n    print(gg)\n    !mkdir image_dir/$gg\n    for i in genre_artist[g]:\n        name = i.replace(\" \", \"_\")\n        if name == 'Albrecht_Dürer':\n            !cp -r /kaggle/input/best-artworks-of-all-time/images/images/Albrecht_Du╠êrer/* 'image_dir/'$gg\n        else:\n            !cp -r '/kaggle/input/best-artworks-of-all-time/images/images/'$name/* 'image_dir/'$gg","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:34:59.470408Z","iopub.execute_input":"2024-03-20T07:34:59.470665Z","iopub.status.idle":"2024-03-20T07:36:21.164438Z","shell.execute_reply.started":"2024-03-20T07:34:59.470643Z","shell.execute_reply":"2024-03-20T07:36:21.163157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mv image_dir/High_Renaissance 'image_dir/High Renaissance'\n!mv image_dir/Northern_Renaissance 'image_dir/Northern Renaissance'\n!mv image_dir/Pop_Art 'image_dir/Pop Art'","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:21.165835Z","iopub.execute_input":"2024-03-20T07:36:21.166150Z","iopub.status.idle":"2024-03-20T07:36:24.075305Z","shell.execute_reply.started":"2024-03-20T07:36:21.166120Z","shell.execute_reply":"2024-03-20T07:36:24.073981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genre_df = data.groupby('genre')['paintings'].sum().reset_index(name='Count')\ngenre_df = genre_df[genre_df['genre'].isin(genre)].reset_index()\ngenre_df = genre_df.drop(['index'],axis = 1)\ngenre_df","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.076893Z","iopub.execute_input":"2024-03-20T07:36:24.077253Z","iopub.status.idle":"2024-03-20T07:36:24.099738Z","shell.execute_reply.started":"2024-03-20T07:36:24.077223Z","shell.execute_reply":"2024-03-20T07:36:24.098744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### add class weight","metadata":{}},{"cell_type":"code","source":"genre_df['class_weight'] = genre_df.Count.sum() / (genre_df.shape[0] * genre_df.Count)\nartists_genre =np.array(genre_df['genre'])  \nartists_genre=np.unique(artists_genre)\n# class_weights = genre_df['class_weight'].to_dict()\nclass_weights = genre_df['class_weight']\nclass_weights = torch.FloatTensor(class_weights).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.101043Z","iopub.execute_input":"2024-03-20T07:36:24.101420Z","iopub.status.idle":"2024-03-20T07:36:24.269542Z","shell.execute_reply.started":"2024-03-20T07:36:24.101393Z","shell.execute_reply":"2024-03-20T07:36:24.268537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### check path","metadata":{}},{"cell_type":"code","source":"images_dir = 'image_dir'\nartists_dirs = os.listdir(images_dir)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.270963Z","iopub.execute_input":"2024-03-20T07:36:24.271617Z","iopub.status.idle":"2024-03-20T07:36:24.276113Z","shell.execute_reply.started":"2024-03-20T07:36:24.271580Z","shell.execute_reply":"2024-03-20T07:36:24.275081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for name in artists_genre:\n    if os.path.exists(os.path.join(images_dir, name)):\n        print(\"Found -->\", os.path.join(images_dir, name))\n    else:\n        print(\"Did not find -->\", os.path.join(images_dir, name))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.277688Z","iopub.execute_input":"2024-03-20T07:36:24.278352Z","iopub.status.idle":"2024-03-20T07:36:24.288293Z","shell.execute_reply.started":"2024-03-20T07:36:24.278320Z","shell.execute_reply":"2024-03-20T07:36:24.287366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"artists_temp = genre_df.copy()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.289589Z","iopub.execute_input":"2024-03-20T07:36:24.289934Z","iopub.status.idle":"2024-03-20T07:36:24.300464Z","shell.execute_reply.started":"2024-03-20T07:36:24.289886Z","shell.execute_reply":"2024-03-20T07:36:24.299554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"genres = [0]*12\nfor i, genre in enumerate(artists_genre):\n    name = artists_genre[i] + '/*'\n    genres[i] = glob.glob(os.path.join(images_dir, name))\n\nfor genre in genres:\n    print(len(genre))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.301590Z","iopub.execute_input":"2024-03-20T07:36:24.302208Z","iopub.status.idle":"2024-03-20T07:36:24.333710Z","shell.execute_reply.started":"2024-03-20T07:36:24.302168Z","shell.execute_reply":"2024-03-20T07:36:24.332881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import glob\n\ndef read_img(file_path):\n    img_arr = cv2.imread(file_path)\n    return cv2.cvtColor(img_arr, cv2.COLOR_BGR2RGB)\n\n\nimg_arrs = []\nimg_num = range(0, 193)\n\n\nfor i in random.sample(img_num,3):\n    img_arrs.append(read_img(genres[0][i]))\n    img_arrs.append(read_img(genres[1][i]))\n    img_arrs.append(read_img(genres[2][i]))\nprint(\"총 {}개의 이미지 \".format(len(img_arrs)))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.334807Z","iopub.execute_input":"2024-03-20T07:36:24.335438Z","iopub.status.idle":"2024-03-20T07:36:24.457036Z","shell.execute_reply.started":"2024-03-20T07:36:24.335413Z","shell.execute_reply":"2024-03-20T07:36:24.456011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rows = 3\ncolumns = 3\n\nfig, axes = plt.subplots(nrows=rows, ncols = columns, figsize=(columns*3, rows*3))\n\nfor num in range(1, rows*columns+1):\n    fig.add_subplot(rows, columns, num)\n    idx = num-1\n    \n    plt.imshow(img_arrs[idx], aspect='auto')\n    plt.xlabel(f'{img_arrs[idx].shape}', fontsize=12)\n    \nfig.tight_layout()\n\ncols = ['1', '2', '3']\nfor folder_idx, ax in enumerate(axes[0]):\n    ax.set_title(cols[folder_idx])\n    \nfor idx, ax in enumerate(axes.flat):\n    ax.set_xticks([])\n    ax.set_yticks([])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:24.458760Z","iopub.execute_input":"2024-03-20T07:36:24.459129Z","iopub.status.idle":"2024-03-20T07:36:27.930413Z","shell.execute_reply.started":"2024-03-20T07:36:24.459095Z","shell.execute_reply":"2024-03-20T07:36:27.929309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(genres[0])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:27.931563Z","iopub.execute_input":"2024-03-20T07:36:27.931870Z","iopub.status.idle":"2024-03-20T07:36:27.937712Z","shell.execute_reply.started":"2024-03-20T07:36:27.931844Z","shell.execute_reply":"2024-03-20T07:36:27.936788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\n\nlen_test_genres = [0]*12\nlen_valid_genres = [0]*12\n\nfor i in range(12):\n    len_test_genres[i] = round(len(genres[i])*0.1)\n    len_valid_genres[i] = round(len(genres[i])*0.1)\n    print(len_test_genres[i], len_valid_genres[i])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:27.938757Z","iopub.execute_input":"2024-03-20T07:36:27.939039Z","iopub.status.idle":"2024-03-20T07:36:27.948681Z","shell.execute_reply.started":"2024-03-20T07:36:27.939015Z","shell.execute_reply":"2024-03-20T07:36:27.947865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\ndef split(img_list, test_count, train_path, test_path, valid_path):\n    test_files = []\n    valid_files = []\n    for i in random.sample(img_list, test_count):\n        test_files.append(i)\n    \n    train_files = [x for x in img_list if x not in test_files]\n    \n    for i in random.sample(train_files, test_count):\n        valid_files.append(i)\n    \n    train_files = [x for x in train_files if x not in valid_files]\n    \n    \n    for k in train_files:\n        shutil.copy(k, train_path)\n    for c in test_files:\n        shutil.copy(c, test_path)\n    for v in valid_files:\n        shutil.copy(v, valid_path)    \n        \n    print('train 폴더 이미지 개수 : {}\\ntest,valid 폴더 이미지 개수 : {}, {}'.format(\n        len(glob.glob(train_path+'/*')), len(glob.glob(test_path+'/*')), len(glob.glob(valid_path+'/*'))\n    ))","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:27.950017Z","iopub.execute_input":"2024-03-20T07:36:27.950321Z","iopub.status.idle":"2024-03-20T07:36:27.959448Z","shell.execute_reply.started":"2024-03-20T07:36:27.950296Z","shell.execute_reply":"2024-03-20T07:36:27.958469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"os.mkdir(os.path.join(images_dir, 'train'))\nos.mkdir(os.path.join(images_dir, 'test'))\nos.mkdir(os.path.join(images_dir, 'valid'))\n\nfor i, genre in enumerate(artists_genre):\n    try:\n        os.mkdir(os.path.join(images_dir, 'train', genre))\n        os.mkdir(os.path.join(images_dir, 'test', genre))\n        os.mkdir(os.path.join(images_dir, 'valid', genre))\n    except:\n        continue","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:27.960423Z","iopub.execute_input":"2024-03-20T07:36:27.960667Z","iopub.status.idle":"2024-03-20T07:36:27.975049Z","shell.execute_reply.started":"2024-03-20T07:36:27.960636Z","shell.execute_reply":"2024-03-20T07:36:27.974246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i, genre in enumerate(artists_genre):\n    train_path = os.path.join(images_dir, 'train', genre)\n    test_path = os.path.join(images_dir, 'test', genre)\n    valid_path = os.path.join(images_dir, 'valid', genre)\n#     print(genre , train_path, test_path)\n    split(genres[i], len_test_genres[i], train_path, test_path, valid_path)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:36:27.976394Z","iopub.execute_input":"2024-03-20T07:36:27.977099Z","iopub.status.idle":"2024-03-20T07:36:29.511901Z","shell.execute_reply.started":"2024-03-20T07:36:27.977052Z","shell.execute_reply":"2024-03-20T07:36:29.510901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## data augmentation","metadata":{}},{"cell_type":"code","source":"train_path = os.path.join(images_dir, 'train')\ntest_path = os.path.join(images_dir, 'test')\nvalid_path = os.path.join(images_dir, 'valid')\ndataset_path = 'image_dir'","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:30.827598Z","iopub.execute_input":"2024-03-20T05:30:30.828283Z","iopub.status.idle":"2024-03-20T05:30:30.833067Z","shell.execute_reply.started":"2024-03-20T05:30:30.828246Z","shell.execute_reply":"2024-03-20T05:30:30.832181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader # 데이터 커스터마이징\nfrom PIL import Image # PIL = Python Image Library\nimport cv2 # albumentation transform을 쓰려면 꼭 이 라이브러리를 이용\nimport tensorflow as tf\n\nclass Custom_Dataset(Dataset):\n    def __init__(self, file_path, mode, transform=None):\n        self.all_data = sorted(glob.glob(os.path.join(file_path, mode, '*', '*')))\n        self.transform = transform\n\n    def __getitem__(self, index):\n        if torch.is_tensor(index):        # 인덱스가 tensor 형태일 수 있으니 리스트 형태로 바꿔준다.\n            index = index.tolist()\n\n        data_path = self.all_data[index]\n        #img = np.array(Image.open(data_path).convert(\"RGB\")) # albumenatation transform을 쓰려면 cv2 라이브러리로 이미지를 읽어야 함\n        image=cv2.imread(data_path)\n        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # BGR -> RGB 변환\n\n        # transform 적용\n        if self.transform is not None:    \n            augmented = self.transform(image=image)\n            image = augmented['image'] \n#             image = image/ 255.0\n#             image = image.to(torch.float32)\n\n        # 이미지 이름을 활용해 label 부여\n        label=[]  \n        \n        for i, g in enumerate(genre_list):\n            if g == data_path.split('/')[2]:\n                label = i\n        return image, label\n\n    def __len__(self):\n        length = len(self.all_data)\n        return length","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:30.834288Z","iopub.execute_input":"2024-03-20T05:30:30.834636Z","iopub.status.idle":"2024-03-20T05:30:42.052080Z","shell.execute_reply.started":"2024-03-20T05:30:30.834602Z","shell.execute_reply":"2024-03-20T05:30:42.051313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations\nimport albumentations.pytorch\n\nalbumentations_resize = albumentations.Compose([\n                                                \n    albumentations.Resize(224,224), \n    albumentations.Normalize(mean=0, std=1),\n    albumentations.pytorch.transforms.ToTensorV2()\n    \n])\n\nresize_train=Custom_Dataset(dataset_path, 'train', transform=albumentations_resize)\nresize_valid=Custom_Dataset(dataset_path, 'valid', transform=albumentations_resize)\nresize_test=Custom_Dataset(dataset_path, 'test', transform=albumentations_resize)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:42.053188Z","iopub.execute_input":"2024-03-20T05:30:42.054530Z","iopub.status.idle":"2024-03-20T05:30:43.264057Z","shell.execute_reply.started":"2024-03-20T05:30:42.054493Z","shell.execute_reply":"2024-03-20T05:30:43.263173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_img(img):    \n    img = img.permute(1,2,0)\n    plt.imshow(img)\n    \nresize_train[488][0].numpy().shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:43.268995Z","iopub.execute_input":"2024-03-20T05:30:43.269652Z","iopub.status.idle":"2024-03-20T05:30:43.311568Z","shell.execute_reply.started":"2024-03-20T05:30:43.269623Z","shell.execute_reply":"2024-03-20T05:30:43.310650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_train[488][0].permute(1,2,0).shape","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:43.312847Z","iopub.execute_input":"2024-03-20T05:30:43.313229Z","iopub.status.idle":"2024-03-20T05:30:47.058649Z","shell.execute_reply.started":"2024-03-20T05:30:43.313191Z","shell.execute_reply":"2024-03-20T05:30:47.057560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_train[488]","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:47.059946Z","iopub.execute_input":"2024-03-20T05:30:47.060238Z","iopub.status.idle":"2024-03-20T05:30:47.111115Z","shell.execute_reply.started":"2024-03-20T05:30:47.060212Z","shell.execute_reply":"2024-03-20T05:30:47.110281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_img(resize_train[488][0])","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:47.112409Z","iopub.execute_input":"2024-03-20T05:30:47.112787Z","iopub.status.idle":"2024-03-20T05:30:47.522180Z","shell.execute_reply.started":"2024-03-20T05:30:47.112753Z","shell.execute_reply":"2024-03-20T05:30:47.521242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import albumentations\nimport albumentations.pytorch\nfrom torchvision import transforms # 이미지 데이터 transform\nfrom torch.utils.data import DataLoader # 이미지 데이터 로더\n\nalbumentations_train = albumentations.Compose([\n                                                \n    albumentations.Resize(224, 224),   \n    albumentations.RandomResizedCrop(224, 224),\n    #albumentations.CenterCrop(224,224),\n    albumentations.OneOf([\n                          albumentations.HorizontalFlip(p=0.8), # p확률로 이미지 좌우 반전\n                          albumentations.RandomRotate90(p=0.8), # p확률로 90도 회전\n                          albumentations.VerticalFlip(p=0.8) # p확률로 이미지 상하 반전\n    ], p=1),\n\n    albumentations.OneOf([\n                          albumentations.MotionBlur(p=0.8), # p확률로 이미지를 흐리게(?) 만들어 줌\n#                           albumentations.OpticalDistortion(p=0.8), # p확률로 이미지 왜곡\n                          albumentations.GaussNoise(p=0.8) # 임의의 noise를 삽입          \n    ], p=1),\n    # albumentations.Normalize(mean = resize_train_mean, std = resize_train_std),\n    albumentations.Normalize(mean=0, std=1),\n    albumentations.pytorch.ToTensorV2()\n    \n])\n\nalbumentations_test = albumentations.Compose([\n                                                \n    albumentations.Resize(224, 224),\n    albumentations.Normalize(mean=0, std=1),\n    albumentations.pytorch.ToTensorV2()\n    \n])\n\n\ntrainset=Custom_Dataset(dataset_path, 'train', transform=albumentations_train)\ntestset=Custom_Dataset(dataset_path, 'test', transform=albumentations_test)\nvalidset=Custom_Dataset(dataset_path, 'valid', transform=albumentations_test)\n\ntrain_loader = torch.utils.data.DataLoader(trainset, batch_size=64,\n                                          shuffle=True, num_workers=0)\n\ntest_loader = torch.utils.data.DataLoader(testset, batch_size=16,\n                                         shuffle=False, num_workers=0)\n\nvalid_loader = torch.utils.data.DataLoader(validset, batch_size=16,\n                                         shuffle=False, num_workers=0)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:30:47.523578Z","iopub.execute_input":"2024-03-20T05:30:47.524171Z","iopub.status.idle":"2024-03-20T05:30:47.562584Z","shell.execute_reply.started":"2024-03-20T05:30:47.524136Z","shell.execute_reply":"2024-03-20T05:30:47.561796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(resize_train[80][0].permute(1,2,0))","metadata":{"execution":{"iopub.status.busy":"2024-03-14T08:04:44.755469Z","iopub.execute_input":"2024-03-14T08:04:44.755935Z","iopub.status.idle":"2024-03-14T08:04:45.176773Z","shell.execute_reply.started":"2024-03-14T08:04:44.755897Z","shell.execute_reply":"2024-03-14T08:04:45.175690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tensor_img(trainset[80][0])","metadata":{"execution":{"iopub.status.busy":"2024-03-14T08:04:39.160949Z","iopub.execute_input":"2024-03-14T08:04:39.161382Z","iopub.status.idle":"2024-03-14T08:04:39.599072Z","shell.execute_reply.started":"2024-03-14T08:04:39.161345Z","shell.execute_reply":"2024-03-14T08:04:39.598059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resize_train[600][0].dtype","metadata":{"execution":{"iopub.status.busy":"2024-03-14T07:59:48.399273Z","iopub.execute_input":"2024-03-14T07:59:48.399853Z","iopub.status.idle":"2024-03-14T07:59:48.421237Z","shell.execute_reply.started":"2024-03-14T07:59:48.399819Z","shell.execute_reply":"2024-03-14T07:59:48.420000Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchvision.models.resnet as resnet\nimport torch.nn as nn\nimport torch.optim as optim\n\nconv1x1 = resnet.conv1x1\nBottleneck = resnet.Bottleneck\nBasicBlock = resnet.BasicBlock","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:48:59.523033Z","iopub.execute_input":"2024-03-13T17:48:59.523636Z","iopub.status.idle":"2024-03-13T17:48:59.528606Z","shell.execute_reply.started":"2024-03-13T17:48:59.523603Z","shell.execute_reply":"2024-03-13T17:48:59.527622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000, zero_init_residual=True):\n        super(ResNet, self).__init__()\n        self.inplanes = 64 #\n\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(block, 64, layers[0], stride=1) # 3 반복\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2) # 4 반복\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2) # 6 반복\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2) # 3 반복\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n\n        # Zero-initialize the last BN in each residual branch,\n        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n        if zero_init_residual:\n            for m in self.modules():\n                if isinstance(m, Bottleneck):\n                    nn.init.constant_(m.bn3.weight, 0)\n                elif isinstance(m, BasicBlock):\n                    nn.init.constant_(m.bn2.weight, 0)\n\n    def _make_layer(self, block, planes, blocks, stride=1): # planes -> 입력되는 채널 수\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion: \n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n\n    def forward(self, x):\n        # input [32, 128, 128] -> [C ,H, W]\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        #x.shape =[32, 64, 64]\n\n        x = self.layer1(x)\n        #x.shape =[128, 64, 64]\n        x = self.layer2(x)\n        #x.shape =[256, 32, 32]\n        x = self.layer3(x)\n        #x.shape =[512, 16, 16]\n        x = self.layer4(x)\n        #x.shape =[1024, 8, 8]\n        \n        x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n\n        return x\n     ","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:48:59.529826Z","iopub.execute_input":"2024-03-13T17:48:59.530127Z","iopub.status.idle":"2024-03-13T17:48:59.548521Z","shell.execute_reply.started":"2024-03-13T17:48:59.530080Z","shell.execute_reply":"2024-03-13T17:48:59.547718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"resnet50 = ResNet(resnet.Bottleneck, [3,4,6,3],12, True).to(device)\n# 1(conv1) + 9(layer1) + 12(layer2) + 18(layer3) + 9(layer4) +1(fc)= ResNet50","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:48:59.549592Z","iopub.execute_input":"2024-03-13T17:48:59.550028Z","iopub.status.idle":"2024-03-13T17:48:59.985023Z","shell.execute_reply.started":"2024-03-13T17:48:59.549996Z","shell.execute_reply":"2024-03-13T17:48:59.984229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install torchsummary","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:48:59.986203Z","iopub.execute_input":"2024-03-13T17:48:59.986911Z","iopub.status.idle":"2024-03-13T17:49:13.174624Z","shell.execute_reply.started":"2024-03-13T17:48:59.986876Z","shell.execute_reply":"2024-03-13T17:49:13.173635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchsummary import summary\nsummary(resnet50, input_size=(3, 224, 224), device=device)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:13.176308Z","iopub.execute_input":"2024-03-13T17:49:13.177204Z","iopub.status.idle":"2024-03-13T17:49:13.893481Z","shell.execute_reply.started":"2024-03-13T17:49:13.177164Z","shell.execute_reply":"2024-03-13T17:49:13.892565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def imshow(img):\n    img = img / 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n     \n# 첫 번째 layer의 filter를 확인해보자 (=가중치 확인) -> 나중에 학습을 완료한 후의 filter도 확인하기\nfor w in resnet50.parameters():\n    w = w.data.cpu()\n    print(w.shape)\n    break\n\n# 가중치 renormalization\nmin_w = torch.min(w)\nw1 = (-1/(2 * min_w)) * w + 0.5\n\n# make grid to display it\ngrid_size = len(w1)\nx_grid = [w1[i] for i in range(grid_size)]\nx_grid = torchvision.utils.make_grid(x_grid, nrow=6, padding=1)\n\nplt.figure(figsize=(10, 10))\nimshow(x_grid)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:13.894821Z","iopub.execute_input":"2024-03-13T17:49:13.895207Z","iopub.status.idle":"2024-03-13T17:49:14.166865Z","shell.execute_reply.started":"2024-03-13T17:49:13.895173Z","shell.execute_reply":"2024-03-13T17:49:14.165984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# config 모델 파라미터 인자를 만들기 위한 클래스\nclass Config:\n    def __init__(self, **kwargs):\n        for key, value in kwargs.items():\n            setattr(self, key, value)\n            \nlr = 0.0008\nepochs = 30\noptimizer = 'Adam'\n\n# 파라미터 클래스\nconfig = Config(\n    trainloader = train_loader,\n    testloader = test_loader,\n    validloader = valid_loader,\n    model = resnet50,\n    device = device,\n    optimizer = torch.optim.Adam(resnet50.parameters(), lr=lr),\n    criterion= nn.CrossEntropyLoss(class_weights).to(device),#class_weights\n    globaliter = 0,\n    patience=10\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:14.168284Z","iopub.execute_input":"2024-03-13T17:49:14.168912Z","iopub.status.idle":"2024-03-13T17:49:14.176743Z","shell.execute_reply.started":"2024-03-13T17:49:14.168875Z","shell.execute_reply":"2024-03-13T17:49:14.175982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import defaultdict\n\ntrain_log = defaultdict(list)\nvalid_log = defaultdict(list)\ntest_log = defaultdict(list)\n\nclass train_test():\n    def __init__(self, config):\n        # 파라미터 인자\n        self.trainloader = config.trainloader\n        self.testloader = config.testloader\n        self.validloader = config.validloader\n        self.model = config.model\n        self.device = config.device\n        self.optimizer = config.optimizer\n        self.criterion = config.criterion\n        self.globaliter = config.globaliter\n        self.patience = config.patience\n        print(len(self.trainloader))\n        \n    def train(self, epochs, log_interval, patience=10):\n        self.model.train()\n        best_loss = float('inf')\n        no_improvement = 0\n    \n        for epoch in range(1, epochs + 1):  \n            running_loss = 0.0\n            running_loss_list = []\n            lr_sche.step()\n    \n            for i, data in enumerate(self.trainloader, 0):\n                self.globaliter += 1\n                inputs, labels = data \n                inputs = inputs.to(self.device)\n                labels = labels.to(self.device)\n    \n                self.optimizer.zero_grad() \n                outputs = self.model(inputs)\n                loss = self.criterion(outputs, labels)\n                loss.backward()\n                self.optimizer.step()\n                running_loss += loss.item()\n    \n                if i % log_interval == log_interval - 1:\n                    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tlearningLoss: {:.6f}\\twhole_loss: {:.6f} '.format(\n                        epoch, i*len(inputs), len(self.trainloader.dataset),\n                        100. * i*len(inputs) / len(self.trainloader.dataset), \n                        running_loss / log_interval,\n                        loss.item()))\n                    train_log['loss'].append(running_loss)\n                    running_loss = 0.0\n\n            # Validation\n            with torch.no_grad():\n                self.model.eval()\n                valid_correct = 0\n                valid_total = 0\n                valid_loss = 0\n                for data in self.validloader:\n                    images, labels = data\n                    images = images.to(self.device)\n                    labels = labels.to(self.device)\n                    outputs = self.model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    valid_total += labels.size(0)\n                    valid_correct += (predicted == labels).sum().item()\n                    valid_loss += self.criterion(outputs, labels).item()\n\n                avg_valid_loss = valid_loss / len(self.validloader)\n                print('\\n{} valid set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%)'.format(\n                      epoch, avg_valid_loss, valid_correct, valid_total, 100 * valid_correct/valid_total))\n                valid_log['loss'].append(avg_valid_loss)\n                valid_log['acc'].append(100 * valid_correct/valid_total)\n                \n            with torch.no_grad():\n                self.model.eval()\n                correct = 0\n                total = 0\n                test_loss = 0\n                for data in self.testloader:\n                    images, labels = data\n                    images = images.to(self.device)\n                    labels = labels.to(self.device)\n                    outputs = self.model(images)\n                    _, predicted = torch.max(outputs.data, 1)\n                    total += labels.size(0)\n                    correct += (predicted == labels).sum().item()\n                    test_loss += self.criterion(outputs, labels).item()\n\n                avg_loss = test_loss / len(self.testloader)\n                print('{} Test set : Average loss:{:.4f}, Accuracy: {}/{}({:.0f}%)\\n'.format(\n                      epoch, avg_loss, correct, total, 100 * correct/total))\n                test_log['loss'].append(avg_loss)\n                test_log['acc'].append(100 * correct/total)\n\n                if avg_valid_loss < best_loss:\n                    best_loss = avg_valid_loss\n                    no_improvement = 0\n                    torch.save(self.model.state_dict(), 'best_model.pt')\n                else:\n                    no_improvement += 1\n                    if no_improvement >= self.patience:\n                        print(\"Early stopping. No improvement for {} epochs.\".format(self.patience))\n                        torch.save(self.model.state_dict(), 'earlystop_model.pt')\n                        return\n\n        print('Finished Training')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:14.178194Z","iopub.execute_input":"2024-03-13T17:49:14.178576Z","iopub.status.idle":"2024-03-13T17:49:14.199546Z","shell.execute_reply.started":"2024-03-13T17:49:14.178544Z","shell.execute_reply":"2024-03-13T17:49:14.198706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ready_to_train=train_test(config)\nlr_sche = optim.lr_scheduler.StepLR(config.optimizer, step_size=100, gamma=0.5) # 20 step마다 lr조정\nepochs = 100\nlog_interval = 50\n\n# ready_to_train.train(epochs, log_interval)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:14.201380Z","iopub.execute_input":"2024-03-13T17:49:14.202046Z","iopub.status.idle":"2024-03-13T17:49:14.213676Z","shell.execute_reply.started":"2024-03-13T17:49:14.202013Z","shell.execute_reply":"2024-03-13T17:49:14.212749Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"############################################################\n#중간","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:14.214727Z","iopub.execute_input":"2024-03-13T17:49:14.215197Z","iopub.status.idle":"2024-03-13T17:49:14.222163Z","shell.execute_reply.started":"2024-03-13T17:49:14.215166Z","shell.execute_reply":"2024-03-13T17:49:14.221316Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = models.resnet50(pretrained=True).to(device)\n\nnum = 0\nfor param in model.parameters():\n    num += 1\n\ncount = 0\nfor param in model.parameters():\n    if count > 50:\n        break\n    param.requires_grad = False\n    count += 1\n    \nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 128),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(128, 12),\n    nn.LogSoftmax(dim=1)\n)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:40:04.094692Z","iopub.execute_input":"2024-03-20T07:40:04.095542Z","iopub.status.idle":"2024-03-20T07:40:05.101394Z","shell.execute_reply.started":"2024-03-20T07:40:04.095494Z","shell.execute_reply":"2024-03-20T07:40:05.100381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## visualize","metadata":{}},{"cell_type":"code","source":"!mkdir filters\n!mkdir filters/train\n!mkdir filters/notrain","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:40:13.623931Z","iopub.execute_input":"2024-03-20T07:40:13.624323Z","iopub.status.idle":"2024-03-20T07:40:16.479295Z","shell.execute_reply.started":"2024-03-20T07:40:13.624292Z","shell.execute_reply":"2024-03-20T07:40:16.478179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_weights = [] # we will save the conv layer weights in this list\nconv_layers = [] # we will save the 49 conv layers in this list\n# get all the model children as list\nmodel_children = list(model.children())\n\n# counter to keep count of the conv layers\ncounter = 0 \n# append all the conv layers and their respective weights to the list\nfor i in range(len(model_children)):\n    if type(model_children[i]) == nn.Conv2d:\n        counter += 1\n        model_weights.append(model_children[i].weight)\n        conv_layers.append(model_children[i])\n    elif type(model_children[i]) == nn.Sequential:\n        for j in range(len(model_children[i])):\n            for child in model_children[i][j].children():\n                if type(child) == nn.Conv2d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\nprint(f\"Total convolutional layers: {counter}\")\n\n# # take a look at the conv layers and the respective weights\n# for weight, conv in zip(model_weights, conv_layers):\n#     # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n#     print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n    \n# visualize the first conv layer filters\nplt.figure(figsize=(20, 17))\nfor i, filter in enumerate(model_weights[0]):\n    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n#     plt.imshow(filter[0, :, :].detach())\n    plt.axis('off')\n    plt.savefig('./filters/notrain/filter.png')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:40:18.274590Z","iopub.execute_input":"2024-03-20T07:40:18.274985Z","iopub.status.idle":"2024-03-20T07:40:32.785695Z","shell.execute_reply.started":"2024-03-20T07:40:18.274950Z","shell.execute_reply":"2024-03-20T07:40:32.784817Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torchvision.transforms as transforms\n\n# Read and visualize an image\nimg = cv2.imread('/kaggle/working/image_dir/Surrealism/Salvador_Dali_139.jpg')\nimg = img.astype(np.uint8)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nplt.show()\n\n# Define the transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n])\n\n# Apply the transforms\nimg = transform(img)\nprint(img.size())\n\n# Unsqueeze to add a batch dimension\nimg = img.unsqueeze(0)\nprint(img.size())\n\n# Move the image tensor to GPU\nimg = img.to(device)\n\n# Pass the image through all the layers\nresults = [conv_layers[0](img)]\nfor i in range(1, len(conv_layers)):\n    # Pass the result from the last layer to the next layer\n    results.append(conv_layers[i](results[-1]))\n\n# Make a copy of the `results`\noutputs = results\n\nfor num_layer in range(len(outputs)):\n    plt.figure(figsize=(30, 30))\n    layer_viz = outputs[num_layer][0, :, :, :]\n    layer_viz = layer_viz.data\n    print(layer_viz.size())\n    for i, filter in enumerate(layer_viz):\n        if i == 64: # we will visualize only 8x8 blocks from each layer\n            break\n        plt.subplot(8, 8, i + 1)\n        plt.imshow(filter.cpu(), cmap='gray')  # Move tensor to CPU for visualization\n        plt.axis(\"off\")\n    print(f\"Saving layer {num_layer} feature maps...\")\n    plt.savefig(f\"./filters/notrain/layer_{num_layer}.png\")\n    plt.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-03-20T07:41:52.846154Z","iopub.execute_input":"2024-03-20T07:41:52.846544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.0001\nepochs = 30\noptimizer = 'Adam'\n\n# 파라미터 클래스\nconfig = Config(\n    trainloader = train_loader,\n    testloader = test_loader,\n    validloader = valid_loader,\n    model = model,\n    device = device,\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr),\n    criterion= nn.CrossEntropyLoss(class_weights).to(device),#class_weights\n    globaliter = 0,\n    patience = 5\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:58.926949Z","iopub.execute_input":"2024-03-13T17:49:58.927621Z","iopub.status.idle":"2024-03-13T17:49:58.934093Z","shell.execute_reply.started":"2024-03-13T17:49:58.927584Z","shell.execute_reply":"2024-03-13T17:49:58.933181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ready_to_train=train_test(config)\nlr_sche = optim.lr_scheduler.StepLR(config.optimizer, step_size=100, gamma=0.5) # 20 step마다 lr조정\nepochs = 100\nlog_interval = 10\n\nready_to_train.train(epochs, log_interval)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:58.935074Z","iopub.execute_input":"2024-03-13T17:49:58.935351Z","iopub.status.idle":"2024-03-13T17:49:58.945959Z","shell.execute_reply.started":"2024-03-13T17:49:58.935329Z","shell.execute_reply":"2024-03-13T17:49:58.945164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## visualize","metadata":{}},{"cell_type":"code","source":"model_weights = [] # we will save the conv layer weights in this list\nconv_layers = [] # we will save the 49 conv layers in this list\n# get all the model children as list\nmodel_children = list(model.children())\n\n# counter to keep count of the conv layers\ncounter = 0 \n# append all the conv layers and their respective weights to the list\nfor i in range(len(model_children)):\n    if type(model_children[i]) == nn.Conv2d:\n        counter += 1\n        model_weights.append(model_children[i].weight)\n        conv_layers.append(model_children[i])\n    elif type(model_children[i]) == nn.Sequential:\n        for j in range(len(model_children[i])):\n            for child in model_children[i][j].children():\n                if type(child) == nn.Conv2d:\n                    counter += 1\n                    model_weights.append(child.weight)\n                    conv_layers.append(child)\nprint(f\"Total convolutional layers: {counter}\")\n\n# # take a look at the conv layers and the respective weights\n# for weight, conv in zip(model_weights, conv_layers):\n#     # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n#     print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")\n    \n# visualize the first conv layer filters\nplt.figure(figsize=(20, 17))\nfor i, filter in enumerate(model_weights[0]):\n    plt.subplot(8, 8, i+1) # (8, 8) because in conv0 we have 7x7 filters and total of 64 (see printed shapes)\n#     plt.imshow(filter[0, :, :].detach())\n    plt.axis('off')\n    plt.savefig('./filters/train/filter.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-03-20T05:28:21.584224Z","iopub.execute_input":"2024-03-20T05:28:21.585046Z","iopub.status.idle":"2024-03-20T05:28:24.443086Z","shell.execute_reply.started":"2024-03-20T05:28:21.585005Z","shell.execute_reply":"2024-03-20T05:28:24.442040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torchvision.transforms as transforms\n\n# Read and visualize an image\nimg = cv2.imread('/kaggle/working/image_dir/Surrealism/Salvador_Dali_139.jpg')\nimg = img.astype(np.uint8)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img)\nplt.show()\n\n# Define the transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((512, 512)),\n    transforms.ToTensor(),\n])\n\n# Apply the transforms\nimg = transform(img)\nprint(img.size())\n\n# Unsqueeze to add a batch dimension\nimg = img.unsqueeze(0)\nprint(img.size())\n\n# Move the image tensor to GPU\nimg = img.to(device)\n\n# Pass the image through all the layers\nresults = [conv_layers[0](img)]\nfor i in range(1, len(conv_layers)):\n    # Pass the result from the last layer to the next layer\n    results.append(conv_layers[i](results[-1]))\n\n# Make a copy of the `results`\noutputs = results\n\nfor num_layer in range(len(outputs)):\n    plt.figure(figsize=(30, 30))\n    layer_viz = outputs[num_layer][0, :, :, :]\n    layer_viz = layer_viz.data\n    print(layer_viz.size())\n    for i, filter in enumerate(layer_viz):\n        if i == 64: # we will visualize only 8x8 blocks from each layer\n            break\n        plt.subplot(8, 8, i + 1)\n        plt.imshow(filter.cpu(), cmap='gray')  # Move tensor to CPU for visualization\n        plt.axis(\"off\")\n    print(f\"Saving layer {num_layer} feature maps...\")\n    plt.savefig(f\"./filters/train/layer_{num_layer}.png\")\n    plt.close()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#######################################################\n#최종","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:58.947781Z","iopub.execute_input":"2024-03-13T17:49:58.948088Z","iopub.status.idle":"2024-03-13T17:49:58.954523Z","shell.execute_reply.started":"2024-03-13T17:49:58.948065Z","shell.execute_reply":"2024-03-13T17:49:58.953717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision import models\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = models.resnet50(pretrained=True).to(device)\n\nmodel.fc = nn.Sequential(\n    nn.Linear(2048, 128),\n    nn.ReLU(),\n    nn.Dropout(0.4),\n    nn.Linear(128, 12),\n    nn.LogSoftmax(dim=1)\n)\n\ncount = 0\nfor param in model.parameters():\n    if count > 0:\n        break\n    param.requires_grad = False\n    count += 1\n    \nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:49:59.093994Z","iopub.execute_input":"2024-03-13T17:49:59.094581Z","iopub.status.idle":"2024-03-13T17:50:00.494615Z","shell.execute_reply.started":"2024-03-13T17:49:59.094552Z","shell.execute_reply":"2024-03-13T17:50:00.493580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr = 0.00008\nepochs = 30\noptimizer = 'Adam'\n\n# 파라미터 클래스\nconfig = Config(\n    trainloader = train_loader,\n    testloader = test_loader,\n    validloader = valid_loader,\n    model = model,\n    device = device,\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr),\n    criterion= nn.CrossEntropyLoss(class_weights).to(device),#class_weights\n    globaliter = 0,\n    patience=10\n)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:50:00.496255Z","iopub.execute_input":"2024-03-13T17:50:00.496600Z","iopub.status.idle":"2024-03-13T17:50:00.503254Z","shell.execute_reply.started":"2024-03-13T17:50:00.496573Z","shell.execute_reply":"2024-03-13T17:50:00.502111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ready_to_train=train_test(config)\nlr_sche = optim.lr_scheduler.StepLR(config.optimizer, step_size=100, gamma=0.5) # 20 step마다 lr조정\nepochs = 100\nlog_interval = 10\n\n#ready_to_train.train(epochs, log_interval)","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:50:00.504427Z","iopub.execute_input":"2024-03-13T17:50:00.504767Z","iopub.status.idle":"2024-03-13T17:53:51.836660Z","shell.execute_reply.started":"2024-03-13T17:50:00.504736Z","shell.execute_reply":"2024-03-13T17:53:51.835160Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from IPython.display import FileLink\nplt.plot(range(len(train_log['loss'])),train_log['loss']) \nplt.savefig('train_loss.png')\n# 다운로드 링크 생성\nFileLink('train_loss.png')","metadata":{"execution":{"iopub.status.busy":"2024-03-13T17:53:57.253573Z","iopub.execute_input":"2024-03-13T17:53:57.254334Z","iopub.status.idle":"2024-03-13T17:53:57.531261Z","shell.execute_reply.started":"2024-03-13T17:53:57.254300Z","shell.execute_reply":"2024-03-13T17:53:57.530356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_log['loss'], label='Test loss')\nplt.plot(valid_log['loss'], label='Validation loss')\n\n# 그래프 제목 및 레이블 설정\nplt.title('Testing, and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\n\n# 범례 표시\nplt.legend()\n\nplt.savefig('testvalid_loss.png')\n# 다운로드 링크 생성\nFileLink('testvalid_loss.png')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.plot(test_log['acc'], label='Test acc')\nplt.plot(valid_log['acc'], label='Validation acc')\n\n# 그래프 제목 및 레이블 설정\nplt.title('Testing, and Validation acc')\nplt.xlabel('Epochs')\nplt.ylabel('acc')\n\n# 범례 표시\nplt.legend()\n\nplt.savefig('testvalid_acc.png')\n# 다운로드 링크 생성\nFileLink('testvalid_acc.png')","metadata":{"execution":{"iopub.status.busy":"2024-03-20T02:59:33.867088Z","iopub.execute_input":"2024-03-20T02:59:33.867446Z","iopub.status.idle":"2024-03-20T02:59:34.242688Z","shell.execute_reply.started":"2024-03-20T02:59:33.867414Z","shell.execute_reply":"2024-03-20T02:59:34.241703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}